{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어 오픈소스 LLM 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n한국어 Fine Tune Llama3 : https://devmeta.tistory.com/80#google_vignette\\n\\nhugging face safetensor -> gguf -> gguf 4bit : https://www.youtube.com/watch?v=jOEu0PE4ozM\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "한국어 Fine Tune Llama3 : https://devmeta.tistory.com/80#google_vignette\n",
    "\n",
    "hugging face safetensor -> gguf -> gguf 4bit : https://www.youtube.com/watch?v=jOEu0PE4ozM\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 48.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\112fk\\\\.cache\\\\huggingface\\\\hub\\\\models--MLP-KTLim--llama-3-Korean-Bllossom-8B\\\\snapshots\\\\8a738f9f622ffc2b0a4a6b81dabbca80406248bf'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 모델 다운로드\n",
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id='MLP-KTLim/llama-3-Korean-Bllossom-8B', local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. GGUF F16으로 convert 시키기\n",
    "'''\n",
    "llama-ccp 레포지토리 : https://github.com/ggerganov/llama.cpp/\n",
    "\n",
    "아나콘다 환경에서 문제 생겨서 wsl2에다가\n",
    "- ubuntu22.04 설치\n",
    "- venv 설치\n",
    "- pytorch 설치 : pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "- torch.cuda.is_available() True 나오는 것 확인\n",
    "- pip instal transformers 설치\n",
    "- 위 코드 터미널 python에서 실행하여 모델들 다운로드(safetensors)\n",
    "- 현재 모델 다운로드 받은 경로 복사 : /home/ubuntu/.cache/huggingface/hub/models--MLP-KTLim--llama-3-Korean-Bllossom-8B/snapshots/8a738f9f622ffc2b0a4a6b81dabbca80406248bf\n",
    "- 들어가서 가상환경 새로 만들고(venv2) requirments.txt 설치\n",
    "- python convert_hf_to_gguf.py 입력하여 명령어 조회\n",
    "- 다음와 같이 수정된 명령어 입력하여 f16으로 변환 : python convert_hf_to_gguf.py --outtype f16 --verbose /path/to/your/model\n",
    "- 해당 경로에 저장되었다고 나옴 : INFO:hf-to-gguf:Model successfully exported to /home/ubuntu/.cache/huggingface/hub/models--MLP-KTLim--llama-3-Korean-Bllossom-8B/snapshots/8a738f9f622ffc2b0a4a6b81dabbca80406248bf/ggml-model-f16.gguf\n",
    "\n",
    "- 이제 4bit로 변환할 순서\n",
    "- https://github.com/ggerganov/llama.cpp/releases 여기 들어가서 llama-b3369-bin-ubuntu-x64.zip 다운로드 받고 wsl2로 unzip해라. 그리고 해당 폴더 안으로 들어가서 아래 실행.\n",
    "\n",
    "ubuntu@nyan-pc:~/llama-quantize$ ./build/bin/llama-quantize /home/ubuntu/.cache/huggingface/hub/models--MLP-KTLim--llama-3-Korean-Bllossom-8B/snaps\n",
    "hots/8a738f9f622ffc2b0a4a6b81dabbca80406248bf/ggml-model-f16.gguf llama3-8b-4bit-q4_1.gguf Q4_1\n",
    "\n",
    "- 이제 4bit 양자화가 제대로 되었다고 뜸.\n",
    "- wsl2에 가상환경 하나 만들어서 ollama 설치하고 아래 블로그 링크 따라하면 잘 동작 한다. 하지만 템플릿이 잘못되었는지 이상함. 확실히 실행 속도는 빠르다.\n",
    "- 하나의 디렉토리에 4bit 양자화된 gguf 파일이랑 토크나이저 넣고 Modelfile 다음 링크와 같이 만들어서 시도 : https://wooiljeong.github.io/ml/gguf-llm/\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama랑 langchain 합쳐서 테스트 중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 기본 채팅\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3-ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='물' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='론' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='이' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='죠' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='!' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 어떤' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 주' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='제' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='로' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 소' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='설' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='을' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 쓰' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='고' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 싶' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='으' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='신' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='가요' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='?' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 판' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='타' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='지' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=',' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 로' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='맨' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='스' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=',' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 미' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='스터' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='리' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=',' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' SF' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 등' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 다양한' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 장' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='르' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 중' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='에서' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 선택' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='해' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content=' 주' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='세요' id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'content='' response_metadata={'model': 'llama3-ko', 'created_at': '2024-07-17T06:16:51.961529Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3407761600, 'load_duration': 2436818000, 'prompt_eval_count': 65, 'prompt_eval_duration': 64745000, 'eval_count': 45, 'eval_duration': 899347000} id='run-e105b5cf-a457-4a9e-81c3-b7ca1d9266d4'"
     ]
    }
   ],
   "source": [
    "input_txt = '긴 소설 하나를 써줄래?'\n",
    "for chunk in llm.stream(input_txt):\n",
    "    print(chunk, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요, 이언님! 무엇을 도와드릴까요', response_metadata={'model': 'llama3-ko', 'created_at': '2024-07-17T06:17:26.9897993Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 554670700, 'load_duration': 21552700, 'prompt_eval_count': 63, 'prompt_eval_duration': 192556000, 'eval_count': 17, 'eval_duration': 337773000}, id='run-b5622049-176e-48ff-8b42-f9586de8f788-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "llm.invoke([HumanMessage(content=\"안녕. 나는 이언이야\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assistant: 안녕하세요! 저녁 식사에 대해 조언해 드리겠습니다. 몇 가지 추천드립니다.\\r\\n1. **한식**: 비빔밥, 불고기, 김치찌개 등 다양한 한식을 선택할 수 있습니다.\\n2. **양식**: 짜장면, 탕수육, 두부 등 맛있는 양식을 즐길 수 있습니다.\\n3. **일식**: 스시, 라멘, 돈까스 등 일본 요리를 추천합니다.\\n4. **중식**: 짬뽕이, 탕원, 마파두프 등 다양한 중식을 선택할 수 있습니다.\\n\\n어떤 종류의 음식을 선호하시는지 알려주시면 더 구체적인 추천을 드릴 수 있습니다'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"안녕. 나는 이언이야\"),\n",
    "        AIMessage(content=\"Assistant: 안녕하세요! 저는 당신의 AI 어시스턴트입니다. 무엇을 도와드릴까요\"),\n",
    "        HumanMessage(content=\"나 배고파 너무. 오늘 저녁에 무엇을 먹으면 좋을까?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assistant: Ah, I apologize for not recalling your previous question. Could you please remind me what it was?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([HumanMessage(content=\"내가 처음에 물어본 것 말이야\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메시지 히스토리 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Assistant: 안녕하세요, 라이언! 어떻게 오늘 하루가 지내고 있나요'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"안녕! 내 이름은 라이언이야\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_llm_end callback: KeyError('message')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Assistant: 안녕하세요! 저는 [Your Name]입니다. 무엇을 도와드릴까요'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"내 이름이 뭐라고?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에러 나서 다른 방법으로 시도 : https://github.com/langchain-ai/langchain/issues/22060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론이죠! 어떤 주제로 소설을 쓰고 싶으신가요? 판타지, 로맨스, 미스터리, SF 등 다양한 장르 중에서 선택해 주세요"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3-ko\")\n",
    "\n",
    "input_txt = '긴 소설 하나를 써줄래?'\n",
    "for chunk in llm.stream(input_txt):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 라이언! 어떻게 도와드릴까요'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"안녕! 내 이름은 라이언이야\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 김철수님! 무엇을 도와드릴까요'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"앞으로 내 이름은 김철수야\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스트리밍 추론 모두 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 김철수님! 무엇을 도와드릴까요\n"
     ]
    }
   ],
   "source": [
    "for chunk in with_message_history.stream([HumanMessage(content=\"앞으로 내 이름은 김철수야\")], config=config):\n",
    "    chunk = chunk.content\n",
    "    print(chunk, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 시도  \n",
    "0. for_rag.txt에 논문이나 긴 글 하나 넣어서 어떤 식으로 검색이 이루어지는지 확인해보기  \n",
    "1. 코사인 유사도로 어느정도 일치하는지 threshold를 만들 수 있는가?  \n",
    "2. stream 구현이 가능한가?(TTS를 위해서는 문장 단위 출력이 필요함)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community.text_splitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerEmbeddings\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community.text_splitter'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from copy import deepcopy\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# 로깅 레벨을 WARNING으로 설정하여 불필요한 메시지 출력 최소화\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# 텍스트 파일 로드 및 전처리\n",
    "text_file_path = \"/path/to/schedule.txt\"  # 파일 경로 설정\n",
    "documents = TextLoader(text_file_path, encoding='utf-8').load()\n",
    "\n",
    "# 문서 청크로 분할\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "\n",
    "# 벡터 저장\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# Q&A 체인 설정\n",
    "llm = ChatOllama(model=\"llama3-ko\")\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "\n",
    "class Ollama_int4_sLLM:\n",
    "    def __init__(self, model_name):\n",
    "        self.model = ChatOllama(model=model_name)\n",
    "        self.store = {}\n",
    "        self.with_message_history = RunnableWithMessageHistory(self.model, self._get_session_history)\n",
    "        self.config_dic = {}\n",
    "        self.response = []\n",
    "        self.instruct = '''사용자가 질문을 하면 이 질문이 실시간으로 인터넷 검색 등을 해야만 알 수 있는 정보인지 아닌지를 판별해서 대답해 주길 바래.\n",
    "        예를 들어 오늘이 무슨 요일인지, 날씨가 어떤지, 오늘은 프로 야구에서 어떤 경기를 하는지 등의 인터넷 연결이 있어야지만 대답할 수 있는 질문에 대해서는 너는 반드시 아무 말이나 지어내지 말고, 인터넷이 연결되어 있지 않아서 답변할 수 없다고 말해.\n",
    "        이런 실시간 인터넷 연결이 필요한 질문에 대해서는 넌 절대 대답을 할 수 없어. 그리고 너가 이미 가지고 있는 상식적인 질문이나 추론적인 질문을 사용자에게 부탁하도록 유도해.\n",
    "        하지만 라면 레시피를 알려 달라던가, 코드를 만들어 달라던가 등의 실시간 인터넷 연결이 필요 없는 너가 이미 알고 있는 정보로 제공할 수 있는 답변은 성실하게 대답해 주도록 해\n",
    "        지금 내가 제시한 예시들에만 적용하여 답변을 하지 말고, 사용자의 질문이 인터넷을 요구하는 질문인지 아닌지를 판단하는데 요점을 두도록 해.\n",
    "        이렇게 해야만 하는 이유는 너는 지금 인터넷에 연결되어 있지 않고, 인터넷 정보가 필요한 대답을 지어서 대답할 경우 잘못된 정보를 사용자에게 제공해서 심각한 손해배성 청구를 받을 수 있기 때문이야.\n",
    "        내가 지도해준 대로 성실히 일을 수행한다면 정말 큰 보상을 주도록 할게\n",
    "        너는 이제부터 키즈케어 로봇이야. 너에게 질문을 하는 사용자는 어린 아이고, 너는 가정 안에서 아이의 정서, 안정, 스케줄 등을 케어하는 역할을 하게 될거야.\n",
    "        이러한 너의 역할을 명시하고 고려하여 답변을 해주길 바래. 어린 아이들은 딱딱하게 대답하지 않고 친구 처럼 반말로 친절하고 장난끼 있게 대답해 주는 것을 좋아해. 절대로 아이에게 존댓말을 하지 말고 반말로 친절하게 대하도록 해. \n",
    "        그리고 설명을 생략하지 말고 모두 답변해되, 너무 길게 하진 마.\n",
    "        '''\n",
    "        self.remove_word_list = ['[사용자 질문]', '[사용자 질문', '[키즈케어 로봇]', '[키즈케어 로봇']\n",
    "\n",
    "    def set_session_id(self, session_id):\n",
    "        config = {'configurable': {'session_id': session_id}}\n",
    "        self.config_dic[session_id] = config\n",
    "        self.invoke(self.instruct, session_id, instruct_mode=True)\n",
    "\n",
    "    def invoke(self, human_message, session_id, ai_name='AI 답변', instruct_mode=False):\n",
    "        self.response = []\n",
    "        sentence = ''\n",
    "        if instruct_mode == False: \n",
    "            print(f'{ai_name} : ', end='')\n",
    "        for chunk in self.with_message_history.stream([HumanMessage(content=human_message)], config=self.config_dic[session_id]):\n",
    "            chunk = chunk.content\n",
    "            sentence = self._remove_words(f'{sentence}{chunk}')\n",
    "            if '.' in chunk:\n",
    "                self.response.append(deepcopy(sentence))\n",
    "                sentence = []\n",
    "            if instruct_mode == False:\n",
    "                print(self._remove_words(chunk), end='')\n",
    "\n",
    "    def auto_chatbot(self, session_id):\n",
    "        print('대화 종료를 위해서는 \"exit()\"를 입력하시오')\n",
    "        print('대화를 시작해주세요.')\n",
    "        while True:\n",
    "            human_message = input(f'\\n{session_id} : ')\n",
    "            if human_message == 'exit()':\n",
    "                print('대화를 종료합니다')\n",
    "                break\n",
    "            self.process_query(human_message, session_id)\n",
    "            \n",
    "    def _get_session_history(self, session_id):\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = InMemoryChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "    \n",
    "    def _remove_words(self, txt):\n",
    "        for word in self.remove_word_list:\n",
    "            txt = txt.replace(word, '').replace('  ', ' ')\n",
    "        return txt\n",
    "\n",
    "    def process_query(self, query, session_id):\n",
    "        # 벡터 유사도 계산 후 필터링\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # 쿼리를 임베딩\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        \n",
    "        # 모든 문서 임베딩 가져오기\n",
    "        all_docs = db.similarity_search(query, k=10)  # 충분히 많은 문서 검색\n",
    "        doc_embeddings = [doc.embedding for doc in all_docs]\n",
    "\n",
    "        # 유사도 계산\n",
    "        similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "        \n",
    "        # 임계값 설정\n",
    "        threshold = 0.8\n",
    "        matching_docs = [doc for doc, sim in zip(all_docs, similarities) if sim >= threshold]\n",
    "\n",
    "        if matching_docs:  # 만약 관련성이 있는 문서가 있다면\n",
    "            answer = qa_chain.run(input_documents=matching_docs, question=query)\n",
    "            print(f'AI 답변: {answer}')\n",
    "        else:  # 관련성이 있는 문서가 없다면 일반적인 대화 진행\n",
    "            self.invoke(query, session_id)\n",
    "\n",
    "session_id = 'Ryan'\n",
    "llm = Ollama_int4_sLLM(model_name='llama3-ko')\n",
    "llm.set_session_id(session_id)\n",
    "\n",
    "# 사용자가 질문할 때 일정에 대해 검색 후 답변 생성\n",
    "llm.auto_chatbot(session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
